{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility in Tensorflow/PyTorch/JAX\n",
    "\n",
    "To illustrate how to set up reproducible training using different frameworks, let's consider the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset as an example. n the following notebook, we'll show you examples of reproducible training in Pytorch, Tensorflow, and JAX, and provide a brief summary of each example. If you are lunching this in [mybinder](https://mybinder.readthedocs.io/en/latest/introduction.html) you will only train on CPU.\n",
    "\n",
    "Firstly we will set some variables. We will only set `CUDA_VISIBLE_DEVICES` to ensure that we are working on one device and `CUBLAS_WORKSPACE_CONFIG`. `CUBLAS_WORKSPACE_CONFIG` maybe needed by Pytorch becouse, bit-wise reproducibility is not guaranteed across toolkit versions because the implementation might differ due to some implementation changes. You will get to choose `:16:8` (may limit overall performance) or `:4096:8` (will increase library footprint in GPU memory by approximately 24MiB). Any of those settings will allow for deterministic behavior even with multiple concurrent streams sharing a single cuBLAS handle.\n",
    "\n",
    "To ensure reproducibility in our deep learning code, we need to set some variables that will control the behavior of our model. Specifically, we will set the `CUDA_VISIBLE_DEVICES` variable to ensure that we are using a specific GPU device, and the `CUBLAS_WORKSPACE_CONFIG` variable to configure cuBLAS, as bit-wise reproducibility is not guaranteed across toolkit versions.\n",
    "\n",
    "The `CUDA_VISIBLE_DEVICES` variable is used to specify which GPU devices are visible to the code. By setting this variable, we can ensure that our code is only using a single GPU device. The `CUBLAS_WORKSPACE_CONFIG` variable can take two values: `:16:8` and `:4096:8`. The `:16:8` setting may limit overall performance, but it allows for deterministic behavior even with multiple concurrent streams sharing a single cuBLAS handle. The `:4096:8` setting will increase the library footprint in GPU memory by approximately 24MiB, but it also allows for deterministic behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set gpu id to test it on one device\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "# You may need to set this variables by PyTorch. It may limit overall performance so be aware of it.\n",
    "%env CUBLAS_WORKSPACE_CONFIG=:16:8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we can import all necessary libraries and set up the environment. But first we will import not machine learning libraries that are also necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we already set some env variables we need to set some additional variables.\n",
    "\n",
    "In addition to the randomness provided by the random module, Python uses a random seed for its hashing algorithm, which affects how objects are stored in sets and dictionaries. This must happen the same way every time in order for GerryChain runs to be repeatable. The way to accomplish this is to set the [environment variable](https://docs.python.org/3.3/using/cmdline.html) `PYTHONHASHSEED`.\n",
    "\n",
    "By utilizing cuDNN, Tensorflow/JAX/PyTorch frameworks can take advantage of the specialized hardware and algorithms provided by NVIDIA GPUs to speed up the training and inference of deep learning models. However the downside of this is lack of reproducibility in some cases so we need to not disable it as it is integrated with CUDA. In Tensorflow now you have a function to make all determinisitc as possible but in previous version you need to do something: In version 1.14, 1.15 and 2.0 you do it by setting `TF_CUDNN_DETERMINISTIC` to `1`, in 2.1-2.8 version you set `TF_CUDNN_USE_FRONTEND` and `TF_DETERMINISTIC_OPS` to `1`. As I am working now on newer version of Tensorflow I am not setting those variables. You can read more about it [here](https://github.com/NVIDIA/framework-determinism/blob/master/doc/tensorflow_status.md).\n",
    "\n",
    "However, for Jax setting `TF_CUDNN_DETERMINISTIC` to `1` allowed me to get reproduciable results so this variable for Jax should be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All frameworks\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "# Jax\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally time for importing our main machine learning modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main imports\n",
    "import random\n",
    "import jax\n",
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# additional imports\n",
    "import torchvision\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax import serialization as flax_serialization\n",
    "from flax.training import train_state, checkpoints\n",
    "import optax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at results when there are no deterministic settings so create an array of 2 by 2 random numbers from a Gaussian normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python numbers\n",
    "python_array = [[random.random() for _ in range(2)] for i in range(2)]\n",
    "print(f\"Python output: {python_array}\")\n",
    "# numpy\n",
    "numpy_array = np.random.rand(2, 2)\n",
    "print(f\"Numpy output: {numpy_array}\")\n",
    "# pytorch\n",
    "pytorch_array = torch.rand(2, 2)\n",
    "print(f\"PyTorch output: {pytorch_array}\")\n",
    "# tensorflow\n",
    "tf_array = tf.random.uniform((2, 2))\n",
    "print(f\"Tensorflow output: {tf_array}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to run this code three times and you will se that each time you will get different results.\n",
    "\n",
    "In order to ensure that our pipeline is reproducible, the final step is to adjust the parameters in our framework.\n",
    "\n",
    "One important consideration when setting these parameters is the use of thread parallelism. Thread parallelism refers to the ability of a computer program to execute multiple threads concurrently. In the context of deep learning, this technique can be used to speed up the training process by allowing different threads to work on different parts of the training data at the same time. This can help to reduce the overall training time and improve the efficiency of the learning algorithm. However this also may influence how reproducible are our results.\n",
    "\n",
    "## Tensorflow\n",
    "\n",
    "To ensure reproducible results in Tensorflow, it's important to properly configure the threading behavior of the runtime. The `tf.config.threading` module provides APIs for configuring the threading behavior of the TensorFlow runtime. The `tf.config.threading.set_inter_op_parallelism_threads` function specifies the number of threads to use for parallel execution of independent operations (also known as \"inter-op\" parallelism), while `tf.config.threading.set_intra_op_parallelism_threads` specifies the number of threads to use for parallel execution of operations within a single op (also known as \"intra-op\" parallelism).\n",
    "\n",
    "In addition to configuring threading behavior, Tensorflow 2.8 introduced a single function for enabling deterministic operations: `tf.config.experimental.enable_op_determinism()`. According to the Tensorflow documentation, when this function is called, \"TensorFlow ops will be deterministic.\"\n",
    "\n",
    "For more information on reproducibility in Tensorflow, you can also check out the resources linked in the original [page]((https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism)): the NVIDIA framework determinism [documentation](https://github.com/NVIDIA/framework-determinism/blob/master/doc/tensorflow.md) and the tensorflow-determinism package on [PyPI](https://pypi.org/project/tensorflow-determinism/).\n",
    "\n",
    "## Pytorch\n",
    "In the case of thread parallelism, you can avoid setting everything to just one thread, as Pytorch gives a nice solution to this problem, which you will explore later.\n",
    "\n",
    "There are also a few configs related to cuDNN that you may want to set in order to ensure reproducibility:\n",
    "- `torch.backends.cudnn.deterministic`: A `bool` that, if set to True, causes cuDNN to only use deterministic convolution algorithms.\n",
    "- `torch.backends.cudnn.benchmark`: A `bool` that, if set to True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest.\n",
    "\n",
    "In addition to these configs, Pytorch provides a function called `torch.use_deterministic_algorithms`, which \"configures PyTorch to use deterministic algorithms instead of nondeterministic ones where available, and to throw an error if an operation is known to be nondeterministic (and without a deterministic alternative).\" This can help to ensure reproducibility of your results.\n",
    "\n",
    "For more information on reproducibility in Pytorch, you can refer to the Pytorch [documentation](https://pytorch.org/docs/stable/notes/randomness.html) on randomness and the NVIDIA framework determinism [documentation](https://github.com/NVIDIA/framework-determinism/blob/master/doc/pytorch.md), which includes a section specifically on Pytorch.\n",
    "\n",
    "## JAX\n",
    "\n",
    "Jax is designed to be deterministic, as long as everything is implemented correctly (To my knowledge). In most cases, XLA used by Jax is deterministic on its own. However, there is an issue with cuDNN that can impact the reproducibility of Jax programs. To address this issue, you can set the `TF_CUDNN_DETERMINISTIC` environment variable to `1`, which will make cuDNN deterministic for Jax.\n",
    "\n",
    "It's worth noting that `TF_CUDNN_DETERMINISTIC` is a Tensorflow environment variable, even though Jax is a separate library. This is because Jax is developed by Google, the same company that developed Tensorflow. As a result, there is some overlap between the two libraries, and you may need to use Tensorflow-specific tools to ensure reproducibility in your Jax programs my padawans 🤓."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Tensorflow ################\n",
    "# Need to restart tensorflow in a hacky way \n",
    "# https://stackoverflow.com/questions/59616436/how-to-reset-initialization-in-tensorflow-2\n",
    "from tensorflow.python.eager import context\n",
    "context._context = None\n",
    "context._create_context()\n",
    "# set the number of threads running on the CPU\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "# set rest of operation to be deterministic\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Pytorch ###################\n",
    "# cudnn settings\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# set rest of operation to be deterministic\n",
    "torch.use_deterministic_algorithms(mode=True, warn_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Devices we will use\")\n",
    "try:\n",
    "    print(torch.cuda.get_device_name(), torch.cuda.device_count())\n",
    "    print(tf.config.list_physical_devices(\"GPU\"))\n",
    "    print(jax.devices())\n",
    "except:\n",
    "    print(\"Well no GPU for you my friend\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to keep in mind when creating a pipeline\n",
    "\n",
    "### Set seed 🌱\n",
    "\n",
    "Remember when you created an array of 2 by 2 random numbers from a Gaussian normal distribution and each time you re-run that cell you would get different results.\n",
    "\n",
    "So now let's set seed. **Remember** you need to set seed for all the used packages and better to always set seed for `random` and `numpy` packages as you don't know what package generate random numbers in some frameworks. Also you will now see that we will also test JAX as it is the only package where you need to always provide RNGs to every random number creation so you always have reproducible results (if you will not screw up somehow), well this solution have some annoying minuses.\n",
    "\n",
    "To demonstrate the importance of setting seeds for reproducibility, let's consider a simple example where we generate random numbers using different packages. If you run this code multiple times, you'll notice that the results are different each time. This is because the default behavior of these packages is to generate random numbers based on the current system time, which means that the numbers will be different every time the code is run (I don't know if it is True chatGPT told me that, but results are the same).\n",
    "\n",
    "To ensure reproducibility, it's important to set seeds for all the packages that generate random numbers. This is especially important for the `random` and `numpy` packages, as you may not always know which package is being used to generate random numbers in a particular framework.\n",
    "\n",
    "One exception to this rule is JAX, which requires you to provide a random number generator (RNG) for every random number generation. This means that you must always keep track of this variable holding the RNG. While this solution is effective, it can be a little cumbersome as you have to provide an RNG for every random number generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for the random number generator\n",
    "seed = 0\n",
    "# seting PRNG for random package\n",
    "random.seed(seed)\n",
    "# seting PRNG for numpy\n",
    "np.random.seed(seed)\n",
    "# seting PRNG for pytorch\n",
    "torch.manual_seed(seed)\n",
    "# if you are on cuda you need to set the seed for the cuda PRNG\n",
    "torch.cuda.manual_seed(seed)\n",
    "# seting PRNG for tensorflow\n",
    "tf.random.set_seed(seed)\n",
    "# In tensorflow you can set seed for Python, NumPy and TensorFlow with one function\n",
    "# tf.keras.utils.set_random_seed(seed)\n",
    "# seting PRNG for jax\n",
    "rng = jax.random.PRNGKey(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now it's time to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python numbers\n",
    "python_array = [[random.random() for _ in range(2)] for i in range(2)]\n",
    "print(f\"Python output: {python_array}\")\n",
    "# numpy\n",
    "numpy_array = np.random.rand(2, 2)\n",
    "print(f\"Numpy output: {numpy_array}\")\n",
    "# pytorch\n",
    "pytorch_array = torch.rand(2, 2)\n",
    "print(f\"PyTorch output: {pytorch_array}\")\n",
    "# tensorflow\n",
    "tf_array = tf.random.uniform((2, 2))\n",
    "print(f\"Tensorflow output: {tf_array}\")\n",
    "# JAX\n",
    "jax_array = jax.random.uniform(rng, (2, 2))\n",
    "print(f\"JAX output: {jax_array}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the code with those seeds multiple times you'll notice that the results are <span style=\"color:green\">consistent</span> each time. This demonstrates the effectiveness of setting seeds to ensure reproducibility.\n",
    "\n",
    "It's worth noting that if you re-run the code in a notebook environment, the results may not be consistent because the code is being run in separate cells. To properly test the reproducibility of the code, you should run it multiple times as a single script or restart the notebook each time.\n",
    "\n",
    "In the case of JAX, it's important to keep track of your RNGs and use them correctly in order to ensure reproducibility. For more information on how to use RNGs in JAX and the advantages of this approach, you can refer to the JAX [documentation](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html) on random numbers.\n",
    "\n",
    "## Datasets and Dataloader\n",
    "\n",
    "As we mentioned earlier, the optimizer updates our deep learning model based on incoming data. If we can make this incoming data deterministic, we can achieve reproducible results from our optimizer.\n",
    "\n",
    "To ensure reproducibility for datasets and data loaders in Pytorch and Tensorflow, you can use separate generators for each. This will ensure that the same dataset and data loader flow are generated each time the code is run, which can be useful for testing and debugging, as well as for ensuring that the results of a machine learning experiment are reproducible.\n",
    "\n",
    "It's important to note that JAX does not have a built-in data loader. However, you can still use the techniques described here to ensure reproducibility for your datasets in JAX.\n",
    "\n",
    "Next, let's take a closer look at the typical usage of datasets and data loaders in Pytorch and Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy dataset with pytorch\n",
    "class DummyPytorchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, shape=(4, 2)):\n",
    "        self.data = torch.rand(*shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "# We create two datasets, one for training and one for testing\n",
    "pytorch_dataset_train = DummyPytorchDataset()\n",
    "pytorch_dataset_test = DummyPytorchDataset()\n",
    "\n",
    "# Create dataloaders\n",
    "# Train dataloader should be shuffled and test dataloader should not be shuffled\n",
    "pytorch_dataloader_train = torch.utils.data.DataLoader(\n",
    "    pytorch_dataset_train, batch_size=2, shuffle=True \n",
    ")\n",
    "pytorch_dataloader_test = torch.utils.data.DataLoader(\n",
    "    pytorch_dataset_test, batch_size=2, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy dataset with tensorflow\n",
    "tf_dataset_train = (\n",
    "    tf.data.Dataset.from_tensor_slices(tf.random.normal((4, 2)))\n",
    "    .cache()\n",
    "    .shuffle(4)\n",
    "    .batch(2)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "tf_dataset_test = (\n",
    "    tf.data.Dataset.from_tensor_slices(tf.random.normal((4, 2)))\n",
    "    .batch(2)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Tensorflow does not have a dataloader, but we can iterate over the dataset or create an iterator\n",
    "# In this case we will use build in function to iterate over the dataset as numpy arrays"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now have a look at the data for the two epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    print(\"Train dataset with shuffle\")\n",
    "    for i, batch in enumerate(pytorch_dataloader_train):\n",
    "        print(f\"Batch id {i} with data {batch}\")\n",
    "\n",
    "    print(\"Test dataset without shuffle\")\n",
    "    for i, batch in enumerate(pytorch_dataloader_test):\n",
    "        print(f\"Batch id {i} with data {batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    print(\"Train dataset with shuffle\")\n",
    "    for i, batch in enumerate(tf_dataset_train.as_numpy_iterator()):\n",
    "        print(f\"Batch id {i} with data {batch}\")\n",
    "\n",
    "    print(\"Test dataset without shuffle\")\n",
    "    for i, batch in enumerate(tf_dataset_test.as_numpy_iterator()):\n",
    "        print(f\"Batch id {i} with data {batch}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the code with previous settings and then restart it, you'll get the same results. However, what if you want to go to a specific checkpoint in your dataset and start from there? If you just load the model weights and optimizers and run the code, you'll get the same data as from the beginning of the first epoch, not the saved data from the checkpoint.\n",
    "\n",
    "In order to reproduce data from a specific checkpoint when training a machine learning model, you can use different approaches depending on the framework you're using. In Tensorflow, you can set the seed for the `shuffle` function for each epoch. In Pytorch, you can use a `torch.Generator` in the `Dataloader`, which handles shuffling and data pre-processing randomness, and can be used to save and load the state of the generator. This allows you to reproduce the data from any point in the training process. To learn more about this technique, you can refer to the Pytorch [documentation](https://pytorch.org/docs/stable/data.html#data-loading-randomness).\n",
    "\n",
    "Now look at the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take care that each worker has consistent seed\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "# Now we create a generator and pass it to the dataloader\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "pytorch_dataloader_train = torch.utils.data.DataLoader(\n",
    "    pytorch_dataset_train, batch_size=2, shuffle=True, worker_init_fn=seed_worker, generator=g,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy dataset with tensorflow without shuffling\n",
    "tf_dataset_train = (\n",
    "    tf.data.Dataset.from_tensor_slices(tf.random.normal((4, 2)))\n",
    "    .cache()\n",
    "    .batch(2)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's see how we can reproduce the same batches of data when shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_rng = g.get_state()\n",
    "for idx, batch in enumerate(pytorch_dataloader_train):\n",
    "    if idx == 0:\n",
    "        print(batch)\n",
    "\n",
    "# now without loading rng state\n",
    "for idx, batch in enumerate(pytorch_dataloader_train):\n",
    "    if idx == 0:\n",
    "        print(batch)\n",
    "\n",
    "# now we load the rng state\n",
    "g.set_state(pytorch_rng)\n",
    "for idx, batch in enumerate(pytorch_dataloader_train):\n",
    "    if idx == 0:\n",
    "        print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_seed = seed\n",
    "epoch = 0\n",
    "for idx, batch in enumerate(tf_dataset_train.shuffle(4, seed=tf_seed).as_numpy_iterator()):\n",
    "    if idx == 0:\n",
    "        print(batch)\n",
    "\n",
    "# now change the seed originally I would do tf_seed+epoch\n",
    "for idx, batch in enumerate(tf_dataset_train.shuffle(4, seed=tf_seed+1).as_numpy_iterator()):\n",
    "    if idx == 0:\n",
    "        print(batch)\n",
    "\n",
    "# now we load the rng state\n",
    "for idx, batch in enumerate(tf_dataset_train.shuffle(4, seed=tf_seed).as_numpy_iterator()):\n",
    "    if idx == 0:\n",
    "        print(batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned earlier, the state of the random number generator (RNG) is important for reproducing the behavior of a dataset at a specific epoch. Without saving the RNG of the generator, we can't go back to a previous epoch and reproduce the dataset behavior because the shuffling is generated from the RNG, and we need to know its state before shuffling to reproduce it correctly.\n",
    "\n",
    "By specifying a seed for the shuffling and transformation functions and incrementally changing it, we can keep track of which stage used which seed/RNG. While this is not a perfect solution, it does allow for reproducible results. I will continue to look for a better solution 😉.\n",
    "\n",
    "To illustrate how to set up reproducible training using different frameworks, let's consider the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset as an example. In the following sections, we'll show you examples of reproducible training in Pytorch, Tensorflow, and JAX, and provide a brief summary of each example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where we dump all the models and data\n",
    "DUMP_MODEL_PATH=\"/tmp/model\"\n",
    "DUMP_DATA_PATH=\"/tmp/data\"\n",
    "\n",
    "os.makedirs(DUMP_MODEL_PATH, exist_ok=True)\n",
    "os.makedirs(DUMP_DATA_PATH, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure once again seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Gnerator for Dataloaders\n",
    "g_train = torch.Generator()\n",
    "g_train.manual_seed(seed)\n",
    "\n",
    "g_test = torch.Generator()\n",
    "g_test.manual_seed(seed)\n",
    "\n",
    "\n",
    "# Parameters for training\n",
    "n_epochs = 4\n",
    "num_workers = 4\n",
    "batch_size_train = 32\n",
    "batch_size_test = 64\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 1000\n",
    "cpu = True # NOTE Change if you want to do on cpu/cuda\n",
    "if cpu:\n",
    "    device = torch.device(\"cpu\") \n",
    "else:\n",
    "    device = torch.device(\"cuda\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST(\n",
    "        DUMP_DATA_PATH,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size_train,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    worker_init_fn=seed_worker, \n",
    "    generator=g_train,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST(\n",
    "        DUMP_DATA_PATH,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size_test,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    worker_init_fn=seed_worker, \n",
    "    generator=g_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    \"\"\"Net class for mnist example\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = torch.nn.Dropout2d()\n",
    "        self.fc1 = torch.nn.Linear(320, 50)\n",
    "        self.fc2 = torch.nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.nn.functional.relu(torch.nn.functional.max_pool2d(self.conv1(x), 2))\n",
    "        x = torch.nn.functional.relu(\n",
    "            torch.nn.functional.max_pool2d(self.conv2_drop(self.conv2(x)), 2)\n",
    "        )\n",
    "        x = x.view(-1, 320)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return torch.nn.functional.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a network and an optimizer\n",
    "network = Net().to(device)\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch: int):\n",
    "    \"\"\"Train the model\n",
    "\n",
    "    Args:\n",
    "        epoch (int): current epoch\n",
    "    \"\"\"\n",
    "    network.train()\n",
    "    start = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data.to(device))\n",
    "        loss = torch.nn.functional.nll_loss(output, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    end_time = time.time() - start            \n",
    "    print(\n",
    "        \"Train Epoch: {} {:.4f}s\\tLoss: {:.6f}\".format(\n",
    "            epoch,\n",
    "            end_time,\n",
    "            loss.item(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def test(epoch: int):\n",
    "    \"\"\"Test the model\n",
    "\n",
    "    Args:\n",
    "        epoch (int): current epoch\n",
    "    \"\"\"\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            target = target.to(device)\n",
    "            output = network(data.to(device))\n",
    "            test_loss += torch.nn.functional.nll_loss(\n",
    "                output, target, reduction='sum'\n",
    "            ).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().detach().cpu()\n",
    "    end_time = time.time() - start\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(\n",
    "        \"Test set: {:.4f}s Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format(\n",
    "            end_time,\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * correct / len(test_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": network.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"torch_rng\": torch.get_rng_state(),\n",
    "            'torch_cuda_rng': 0 if cpu else torch.cuda.get_rng_state(),\n",
    "            \"numpy_rng\": np.random.get_state(),\n",
    "            \"python_state\": random.getstate(),\n",
    "            \"generator_dataloader_train\": g_train.get_state(),\n",
    "            \"generator_dataloader_test\": g_test.get_state()\n",
    "        },\n",
    "        f\"{DUMP_MODEL_PATH}/pytorch_model_{epoch}.pth\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train the model\n",
    "test(0)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure our model is loaded not from cache\n",
    "del network, optimizer\n",
    "\n",
    "# create a new network and optimizer\n",
    "network = Net().to(device)\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# Load last checkpoint\n",
    "checkpoint = torch.load(f\"{DUMP_MODEL_PATH}/pytorch_model_{n_epochs}.pth\")\n",
    "epoch_last = checkpoint[\"epoch\"]\n",
    "network.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "torch.set_rng_state(checkpoint[\"torch_rng\"])\n",
    "if not cpu:\n",
    "    torch.cuda.set_rng_state(checkpoint['torch_cuda_rng'])\n",
    "g_train.set_state(checkpoint[\"generator_dataloader_train\"])\n",
    "g_test.set_state(checkpoint[\"generator_dataloader_test\"])\n",
    "np.random.set_state(checkpoint[\"numpy_rng\"])\n",
    "random.setstate(checkpoint[\"python_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(epoch_last)\n",
    "for epoch in range(epoch_last + 1, n_epochs + 3):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure our model is loaded not from cache\n",
    "del network, optimizer\n",
    "\n",
    "# create a new network and optimizer\n",
    "network = Net().to(device)\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# Load second checkpoint\n",
    "checkpoint = torch.load(f\"{DUMP_MODEL_PATH}/pytorch_model_2.pth\")\n",
    "epoch_last = checkpoint[\"epoch\"]\n",
    "network.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "torch.set_rng_state(checkpoint[\"torch_rng\"])\n",
    "if not cpu:\n",
    "    torch.cuda.set_rng_state(checkpoint['torch_cuda_rng'])\n",
    "g_train.set_state(checkpoint[\"generator_dataloader_train\"])\n",
    "g_test.set_state(checkpoint[\"generator_dataloader_test\"])\n",
    "np.random.set_state(checkpoint[\"numpy_rng\"])\n",
    "random.setstate(checkpoint[\"python_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(epoch_last)\n",
    "for epoch in range(epoch_last + 1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary time:\n",
    "- Can we reproduce the results by restarting the script for CPU? Answer ✅.\n",
    "- Can we reproduce the results by restarting the script for the GPU? Answer ✅\n",
    "- Can we reproduce the results from the checkpoints? Answer ✅\n",
    "- Can we reproduce results on CPU and GPU (same device)? Answer ❌\n",
    "- Can we reproduce results on different CPUs? Answer ❌\n",
    "- Can we reproduce results on different but same GPU? Answer ✅\n",
    "- Can we reproduce the results on different graphics cards? Answer ❌ (but I will check this on a few different cards)\n",
    "\n",
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure once again seed\n",
    "tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "# Parameters for training\n",
    "n_epochs = 4\n",
    "batch_size_train = 16\n",
    "batch_size_test = 32\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Tensorflow dataset / Keras dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data(path=f\"{DUMP_DATA_PATH}/mnist.npz\")\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Make train and test data loaders\n",
    "train_loader = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "test_loader = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    .batch(batch_size_test)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Keras model\n",
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Reshape((28,28,-1)),\n",
    "        tf.keras.layers.Conv2D(filters=10, kernel_size=5),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Conv2D(filters=20, kernel_size=5),\n",
    "        # sorry I did have small error when I used dropout here so I just removed it\n",
    "        # It will not change enything just for comparison between architectures it will\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "# Compile model\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum)\n",
    "\n",
    "# Create checkpoint callback\n",
    "cp = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "\n",
    "# Metrics\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
    "test_loss = tf.keras.metrics.Mean(name=\"test_loss\")\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"test_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(data, target):\n",
    "    \"\"\"Train function\n",
    "    \n",
    "    Args:\n",
    "        data: input data\n",
    "        target: target for the input data\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(data, training=True)\n",
    "        loss = loss_fn(target, predictions)\n",
    "    grad = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grad, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(target, predictions)\n",
    "    return\n",
    "\n",
    "@tf.function\n",
    "def test_step(data, target):\n",
    "    \"\"\"Train function\n",
    "    \n",
    "    Args:\n",
    "        data: input data\n",
    "        target: target for the input data\n",
    "    \"\"\"\n",
    "    predictions = model(data, training=False)\n",
    "    t_loss = loss_fn(target, predictions)\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(target, predictions)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "test_loss.reset_states()\n",
    "test_accuracy.reset_states()\n",
    "for images, labels in test_loader:\n",
    "    test_step(images, labels)\n",
    "print(\n",
    "    f\"Epoch {0}, \"\n",
    "    f\"Test Loss: {test_loss.result():.4f}, \"\n",
    "    f\"Test Accuracy: {test_accuracy.result():.4f}\"\n",
    ")\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    # Reset trackers for metrics\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    dl_seed = seed + epoch\n",
    "\n",
    "    # Train\n",
    "    for batch, (images, labels) in enumerate(train_loader.shuffle(len(x_train), seed=dl_seed).batch(batch_size_train)):\n",
    "        train_step(images, labels)\n",
    "    \n",
    "    # Save\n",
    "    cp.write(f\"{DUMP_MODEL_PATH}/tf_models_{epoch}.h5\")\n",
    "    \n",
    "    # Test\n",
    "    for images, labels in test_loader:\n",
    "        test_step(images, labels)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}, \"\n",
    "        f\"Loss: {train_loss.result():.4f}, \"\n",
    "        f\"Accuracy: {train_accuracy.result():.4f}, \"\n",
    "        f\"Test Loss: {test_loss.result():.4f}, \"\n",
    "        f\"Test Accuracy: {test_accuracy.result():.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load last epoch\n",
    "cp.restore(f\"{DUMP_MODEL_PATH}/tf_models_{n_epochs}.h5\")\n",
    "\n",
    "# Evaluate\n",
    "test_loss.reset_states()\n",
    "test_accuracy.reset_states()\n",
    "for images, labels in test_loader:\n",
    "    test_step(images, labels)\n",
    "print(\n",
    "    f\"Epoch {n_epochs}, \"\n",
    "    f\"Test Loss: {test_loss.result():.4f}, \"\n",
    "    f\"Test Accuracy: {test_accuracy.result():.4f}\"\n",
    ")\n",
    "\n",
    "for epoch in range(n_epochs + 1, n_epochs + 3):\n",
    "    # Reset trackers for metrics\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    dl_seed = seed + epoch\n",
    "\n",
    "    # Train\n",
    "    for batch, (images, labels) in enumerate(train_loader.shuffle(len(x_train), seed=dl_seed).batch(batch_size_train)):\n",
    "        train_step(images, labels)\n",
    "    \n",
    "    # Save\n",
    "    cp.write(f\"{DUMP_MODEL_PATH}/tf_models_{epoch}.h5\")\n",
    "    \n",
    "    # Test\n",
    "    for images, labels in test_loader:\n",
    "        test_step(images, labels)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}, \"\n",
    "        f\"Loss: {train_loss.result():.4f}, \"\n",
    "        f\"Accuracy: {train_accuracy.result():.4f}, \"\n",
    "        f\"Test Loss: {test_loss.result():.4f}, \"\n",
    "        f\"Test Accuracy: {test_accuracy.result():.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load second epoch\n",
    "cp.restore(\"{DUMP_MODEL_PATH}/tf_models_2.h5\")\n",
    "\n",
    "# Evaluate\n",
    "test_loss.reset_states()\n",
    "test_accuracy.reset_states()\n",
    "for images, labels in test_loader:\n",
    "    test_step(images, labels)\n",
    "print(\n",
    "    f\"Epoch {2}, \"\n",
    "    f\"Test Loss: {test_loss.result():.4f}, \"\n",
    "    f\"Test Accuracy: {test_accuracy.result():.4f}\"\n",
    ")\n",
    "\n",
    "for epoch in range(2 + 1, n_epochs + 1):\n",
    "    # Reset trackers for metrics\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    dl_seed = seed + epoch\n",
    "\n",
    "    # Train\n",
    "    for batch, (images, labels) in enumerate(train_loader.shuffle(len(x_train), seed=dl_seed).batch(batch_size_train)):\n",
    "        train_step(images, labels)\n",
    "    \n",
    "    # Save\n",
    "    cp.write(f\"{DUMP_MODEL_PATH}/tf_models_{epoch}.h5\")\n",
    "    \n",
    "    # Test\n",
    "    for images, labels in test_loader:\n",
    "        test_step(images, labels)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}, \"\n",
    "        f\"Loss: {train_loss.result():.4f}, \"\n",
    "        f\"Accuracy: {train_accuracy.result():.4f}, \"\n",
    "        f\"Test Loss: {test_loss.result():.4f}, \"\n",
    "        f\"Test Accuracy: {test_accuracy.result():.4f}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary time:\n",
    "- Can we reproduce the results by restarting the script for CPU? Answer ✅.\n",
    "- Can we reproduce the results by restarting the script for the GPU? Answer ✅\n",
    "- Can we reproduce the results from the checkpoints? Answer ❌\n",
    "- Can we reproduce results on CPU and GPU (same device)? Answer ❌\n",
    "- Can we reproduce results on different CPUs? Answer ❌\n",
    "- Can we reproduce results on different but same GPU? Answer ✅\n",
    "- Can we reproduce the results on different graphics cards? Answer ❌ (but I will check this on a few different cards)\n",
    "\n",
    "## JAX\n",
    "\n",
    "For our example of reproducible training in JAX, we'll be using the [Flax](https://flax.readthedocs.io/en/latest/) library to build our deep neural network (DNN). Flax is an overlay library built on top of JAX that provides a more intuitive interface for building DNNs. It does not add any additional functionality to JAX, but it makes it easier to build DNNs using JAX, which was not designed specifically for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure once again seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "rng = jax.random.PRNGKey(seed)\n",
    "\n",
    "# Gnerator for Dataloaders\n",
    "g_train = torch.Generator()\n",
    "g_train.manual_seed(seed)\n",
    "\n",
    "g_test = torch.Generator()\n",
    "g_test.manual_seed(seed)\n",
    "\n",
    "# Parameters for training\n",
    "n_epochs = 4\n",
    "num_workers = 4\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  \"\"\"A simple CNN model.\"\"\"\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x, training=False):\n",
    "    x = nn.Conv(features=10, kernel_size=(5, 5))(x)\n",
    "    x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Conv(features=20, kernel_size=(5, 5))(x)\n",
    "    x = nn.Dropout(0.2)(x, deterministic=not training)\n",
    "    x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = nn.relu(x)\n",
    "    x = x.reshape((x.shape[0], -1))  # flatten\n",
    "    x = nn.Dense(features=50)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dropout(0.2)(x, deterministic=not training)\n",
    "    x = nn.Dense(features=10)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, labels):\n",
    "    \"\"\"Compute the cross-entropy loss given logits and labels.\n",
    "\n",
    "    Args:\n",
    "        logits: output of the model\n",
    "        labels: labels of the data\n",
    "    \"\"\"\n",
    "    labels_onehot = jax.nn.one_hot(labels, num_classes=10)\n",
    "    return optax.softmax_cross_entropy(logits=logits, labels=labels_onehot).mean()\n",
    "\n",
    "\n",
    "def compute_metrics_jax(logits, labels):\n",
    "    \"\"\"Compute metrics for the model.\n",
    "\n",
    "    Args:\n",
    "        logits: output of the model\n",
    "        labels: labels of the data\n",
    "    \"\"\"\n",
    "    loss = cross_entropy_loss(logits=logits, labels=labels)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    return {'loss': loss, 'accuracy': accuracy}\n",
    "\n",
    "\n",
    "def create_train_state(rng, learning_rate, momentum):\n",
    "    \"\"\"Creates initial `TrainState` with `sgd`.\"\"\"\n",
    "    cnn = CNN()\n",
    "    rng, rng_dropout = jax.random.split(rng)\n",
    "    params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params'] # initialize parameters by passing a template image\n",
    "    tx = optax.sgd(learning_rate, momentum)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=cnn.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, image, label, dropout_rng):\n",
    "  \"\"\"Train for a single step. Also jit-compiled for speed.\n",
    "  \n",
    "  Args:\n",
    "    params: parameters of the model\n",
    "    image: input image\n",
    "    label: label of the image\n",
    "    dropout_rng: rng for dropout\n",
    "  \"\"\"\n",
    "  def loss_fn(params):\n",
    "    logits = CNN().apply({'params': params}, x=image, training=True, rngs={'dropout': dropout_rng})\n",
    "    loss = cross_entropy_loss(logits=logits, labels=label)\n",
    "    return loss, logits\n",
    "  grad_fn = jax.grad(loss_fn, has_aux=True)\n",
    "  grads, logits = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  metrics = compute_metrics_jax(logits=logits, labels=label)\n",
    "  return state, metrics\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(params, image, label):\n",
    "  \"\"\"Evaluate for a single step. Also jit-compiled for speed.\n",
    "  \n",
    "  Args:\n",
    "    params: parameters of the model\n",
    "    image: input image\n",
    "    label: label of the image\n",
    "  \"\"\"\n",
    "  logits = CNN().apply({'params': params}, image)\n",
    "  return compute_metrics_jax(logits=logits, labels=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(state, train_ds, epoch, rng):\n",
    "  \"\"\"Train for a single epoch.\n",
    "  \n",
    "  Args:\n",
    "    params: model parameters\n",
    "    test_ds: test dataloader\n",
    "    epoch: current epoch\n",
    "  \"\"\"\n",
    "  batch_metrics = []\n",
    "  start = time.time()\n",
    "  for batch_idx, (data, target) in enumerate(train_ds):\n",
    "    rng, rng_dropout = jax.random.split(rng)\n",
    "    state, metrics = train_step(state, data, target, rng_dropout)\n",
    "    batch_metrics.append(metrics)\n",
    "\n",
    "  # compute mean of metrics across each batch in epoch.\n",
    "  batch_metrics_np = jax.device_get(batch_metrics)\n",
    "  epoch_metrics_np = {\n",
    "      k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "      for k in batch_metrics_np[0]} # jnp.mean does not work on lists\n",
    "\n",
    "  end_time = time.time() - start\n",
    "  print('train epoch: %d, time %.4f, loss: %.4f, accuracy: %.2f' % (\n",
    "      epoch, end_time, epoch_metrics_np['loss'], epoch_metrics_np['accuracy'] * 100))\n",
    "  return state, rng\n",
    "\n",
    "\n",
    "def eval_model(params, test_ds):\n",
    "  \"\"\"Evaluate the model.\n",
    "  \n",
    "  Args:\n",
    "    params: model parameters\n",
    "    test_ds: test dataloader\n",
    "  \"\"\"\n",
    "  batch_metrics = []\n",
    "  start = time.time()\n",
    "  for batch_idx, (data, target) in enumerate(test_ds):\n",
    "    metrics = eval_step(params, data, target)\n",
    "    batch_metrics.append(metrics)\n",
    "    \n",
    "  # compute mean of metrics across each batch in epoch.\n",
    "  batch_metrics_np = jax.device_get(batch_metrics)\n",
    "  metrics = {\n",
    "      k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "      for k in batch_metrics_np[0]} # jnp.mean does not work on lists\n",
    "  end_time = time.time() - start\n",
    "  summary = jax.tree_util.tree_map(lambda x: x.item(), metrics) # map the function over all leaves in metrics\n",
    "  return summary['loss'], summary['accuracy'], end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations applied on each image => bring them into a numpy array\n",
    "def image_to_numpy(img):\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    return img\n",
    "\n",
    "# We need to stack the batch elements\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST(\n",
    "        DUMP_DATA_PATH,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                image_to_numpy\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size_train,\n",
    "    shuffle=True,\n",
    "    collate_fn=numpy_collate,\n",
    "    num_workers=num_workers,\n",
    "    worker_init_fn=seed_worker, \n",
    "    generator=g_train,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST(\n",
    "        DUMP_DATA_PATH,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                image_to_numpy\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size_test,\n",
    "    shuffle=False,\n",
    "    collate_fn=numpy_collate,\n",
    "    num_workers=num_workers,\n",
    "    worker_init_fn=seed_worker, \n",
    "    generator=g_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng, init_rng = jax.random.split(rng)\n",
    "state = create_train_state(init_rng, learning_rate, momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy, end_time = eval_model(state.params, test_loader)\n",
    "print('test epoch: %d, time: %.4f, loss: %.2f, accuracy: %.2f' % (\n",
    "      0, end_time, test_loss, test_accuracy * 100))\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  state, rng = train_epoch(state, train_loader, epoch, rng)\n",
    "  # Evaluate on the test set after each training epoch\n",
    "  test_loss, test_accuracy, end_time = eval_model(state.params, test_loader)\n",
    "  print(' test epoch: %d, time: %.4f, loss: %.2f, accuracy: %.2f' % (\n",
    "      epoch, end_time, test_loss, test_accuracy * 100))\n",
    "  # Save the model parameters\n",
    "  numpy_rng = np.random.get_state()\n",
    "  python_state = random.getstate()\n",
    "  torch_state = torch.get_rng_state() # well only this is required but for the sake of completeness I am adding all\n",
    "  checkpoints.save_checkpoint(\n",
    "      ckpt_dir=f\"{DUMP_MODEL_PATH}/jax_checkpoints_{epoch}\",\n",
    "      target={\"state\": state,\n",
    "              \"epoch\": epoch,\n",
    "              \"rng\": rng,\n",
    "              \"pytorch_rng\": torch_state.numpy(),\n",
    "              \"numpy_rng\": numpy_rng,\n",
    "              \"python_state\": python_state,\n",
    "              \"generator_dataloader_train\": g_train.get_state().numpy(),\n",
    "              \"generator_dataloader_test\": g_test.get_state().numpy(),\n",
    "            },\n",
    "            step=epoch,\n",
    "            overwrite=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model parameters\n",
    "load_dict = checkpoints.restore_checkpoint(ckpt_dir=f\"{DUMP_MODEL_PATH}/jax_checkpoints_{n_epochs}\", target=None)\n",
    "rng = load_dict[\"rng\"]\n",
    "torch.set_rng_state(torch.tensor(load_dict[\"pytorch_rng\"]))\n",
    "python_state = list(load_dict[\"python_state\"].values())\n",
    "python_state[1] = tuple(python_state[1].values())\n",
    "g_train.set_state(torch.tensor(load_dict[\"generator_dataloader_train\"]))\n",
    "g_test.set_state(torch.tensor(load_dict[\"generator_dataloader_test\"]))\n",
    "np.random.set_state(tuple(load_dict[\"numpy_rng\"].values()))\n",
    "random.setstate(python_state)\n",
    "epoch_last = load_dict[\"epoch\"]\n",
    "state = flax_serialization.from_state_dict(state, load_dict['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy, end_time = eval_model(state.params, test_loader)\n",
    "print(' test epoch: %d, time: %.4f, loss: %.2f, accuracy: %.2f' % (\n",
    "      epoch_last, end_time, test_loss, test_accuracy * 100))\n",
    "\n",
    "for epoch in range(epoch_last+1, n_epochs + 3):\n",
    "  state, rng = train_epoch(state, train_loader, epoch, rng)\n",
    "  # Evaluate on the test set after each training epoch\n",
    "  test_loss, test_accuracy, end_time = eval_model(state.params, test_loader)\n",
    "  print(' test epoch: %d, time: %.4f, loss: %.2f, accuracy: %.2f' % (\n",
    "      epoch, end_time, test_loss, test_accuracy * 100))\n",
    "  # Save the model parameters\n",
    "  numpy_rng = np.random.get_state()\n",
    "  python_state = random.getstate()\n",
    "  torch_state = torch.get_rng_state() # well only this is required but for the sake of completeness I am adding all\n",
    "  checkpoints.save_checkpoint(\n",
    "      ckpt_dir=f\"{DUMP_MODEL_PATH}/jax_checkpoints_{epoch}\",\n",
    "      target={\"state\": state,\n",
    "              \"epoch\": epoch,\n",
    "              \"rng\": rng,\n",
    "              \"pytorch_rng\": torch_state.numpy(),\n",
    "              \"numpy_rng\": numpy_rng,\n",
    "              \"python_state\": python_state,\n",
    "              \"generator_dataloader_train\": g_train.get_state().numpy(),\n",
    "              \"generator_dataloader_test\": g_test.get_state().numpy(),\n",
    "            },\n",
    "            step=epoch,\n",
    "            overwrite=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the second model parameters\n",
    "load_dict = checkpoints.restore_checkpoint(ckpt_dir=f\"{DUMP_MODEL_PATH}/jax_checkpoints_2\", target=None)\n",
    "rng = load_dict[\"rng\"]\n",
    "torch.set_rng_state(torch.tensor(load_dict[\"pytorch_rng\"]))\n",
    "python_state = list(load_dict[\"python_state\"].values())\n",
    "python_state[1] = tuple(python_state[1].values())\n",
    "g_train.set_state(torch.tensor(load_dict[\"generator_dataloader_train\"]))\n",
    "g_test.set_state(torch.tensor(load_dict[\"generator_dataloader_test\"]))\n",
    "np.random.set_state(tuple(load_dict[\"numpy_rng\"].values()))\n",
    "random.setstate(python_state)\n",
    "epoch_last = load_dict[\"epoch\"]\n",
    "state = flax_serialization.from_state_dict(state, load_dict['state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy, end_time = eval_model(state.params, test_loader)\n",
    "print(' test epoch: %d, time: %.4f, loss: %.2f, accuracy: %.2f' % (\n",
    "      epoch_last, end_time, test_loss, test_accuracy * 100))\n",
    "\n",
    "for epoch in range(epoch_last+1, n_epochs + 1):\n",
    "  state, rng = train_epoch(state, train_loader, epoch, rng)\n",
    "  # Evaluate on the test set after each training epoch\n",
    "  test_loss, test_accuracy, end_time = eval_model(state.params, test_loader)\n",
    "  print(' test epoch: %d, time: %.4f, loss: %.2f, accuracy: %.2f' % (\n",
    "      epoch, end_time, test_loss, test_accuracy * 100))\n",
    "  # Save the model parameters\n",
    "  numpy_rng = np.random.get_state()\n",
    "  python_state = random.getstate()\n",
    "  torch_state = torch.get_rng_state() # well only this is required but for the sake of completeness I am adding all\n",
    "  checkpoints.save_checkpoint(\n",
    "      ckpt_dir=f\"{DUMP_MODEL_PATH}/jax_checkpoints_{epoch}\",\n",
    "      target={\"state\": state,\n",
    "              \"epoch\": epoch,\n",
    "              \"rng\": rng,\n",
    "              \"pytorch_rng\": torch_state.numpy(),\n",
    "              \"numpy_rng\": numpy_rng,\n",
    "              \"python_state\": python_state,\n",
    "              \"generator_dataloader_train\": g_train.get_state().numpy(),\n",
    "              \"generator_dataloader_test\": g_test.get_state().numpy(),\n",
    "            },\n",
    "            step=epoch,\n",
    "            overwrite=True,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary time:\n",
    "- Can we reproduce the results by restarting the script for CPU? Answer ✅.\n",
    "- Can we reproduce the results by restarting the script for the GPU? Answer ✅\n",
    "- Can we reproduce the results from the checkpoints? Answer ✅\n",
    "- Can we reproduce results on CPU and GPU (same device)? Answer ❌\n",
    "- Can we reproduce results on different CPUs? Answer ❌\n",
    "- Can we reproduce results on different but same GPU? Answer ✅\n",
    "- Can we reproduce the results on different graphics cards? Answer ❌ (but I will check this on a few different cards)\n",
    "\n",
    "# Before you leave remember to clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5 (main, Jun  8 2022, 09:26:22) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1dbd6313390e1b7febcd4a77298bdfe585afae5cdaaeaa3cee8a758ff684baa4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
